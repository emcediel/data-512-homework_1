{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1b3d869",
   "metadata": {},
   "source": [
    "# Step 1: Data Adqiuisition ##\n",
    "## Obtaining data from API\n",
    "We access page view data using the [Wikimedia REST API](https://www.mediawiki.org/wiki/Wikimedia_REST_API). We request monthly counts of page views for multiple articles. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87ace9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are standard python modules\n",
    "import json, time, urllib.parse, pandas as pd\n",
    "#\n",
    "# The 'requests' module is not a standard Python module. You will need to install this with pip/pip3 if you do not already have it\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc1e4d3",
   "metadata": {},
   "source": [
    "We declare all necessary constants for the request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2237f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "#\n",
    "#    CONSTANTS\n",
    "#\n",
    "\n",
    "# The REST API 'pageviews' URL - this is the common URL/endpoint for all 'pageviews' API requests\n",
    "API_REQUEST_PAGEVIEWS_ENDPOINT = 'https://wikimedia.org/api/rest_v1/metrics/pageviews/'\n",
    "\n",
    "# This is a parameterized string that specifies what kind of pageviews request we are going to make\n",
    "# In this case it will be a 'per-article' based request. The string is a format string so that we can\n",
    "# replace each parameter with an appropriate value before making the request\n",
    "API_REQUEST_PER_ARTICLE_PARAMS = 'per-article/{project}/{access}/{agent}/{article}/{granularity}/{start}/{end}'\n",
    "\n",
    "# The Pageviews API asks that we not exceed 100 requests per second, we add a small delay to each request\n",
    "API_LATENCY_ASSUMED = 0.002       # Assuming roughly 2ms latency on the API and network\n",
    "API_THROTTLE_WAIT = (1.0/100.0)-API_LATENCY_ASSUMED\n",
    "\n",
    "# When making a request to the Wikimedia API they ask that you include a \"unique ID\" that will allow them to\n",
    "# contact you if something happens - such as - your code exceeding request limits - or some other error happens\n",
    "REQUEST_HEADERS = {\n",
    "    'User-Agent': '<uwnetid@uw.edu>, University of Washington, MSDS DATA 512 - AUTUMN 2022',\n",
    "}\n",
    "\n",
    "# This template is used to map parameter values into the API_REQUST_PER_ARTICLE_PARAMS portion of an API request. The dictionary has a\n",
    "# field/key for each of the required parameters. In the example, below, we only vary the article name, so the majority of the fields\n",
    "# can stay constant for each request. Of course, these values *could* be changed if necessary.\n",
    "ARTICLE_PAGEVIEWS_PARAMS_TEMPLATE = {\n",
    "    \"project\":     \"en.wikipedia.org\",\n",
    "    \"access\":      \"desktop\",      # this should be changed for the different access types\n",
    "    \"agent\":       \"user\",\n",
    "    \"article\":     \"\",             # this value will be set/changed before each request\n",
    "    \"granularity\": \"monthly\",\n",
    "    \"start\":       \"2015070100\",\n",
    "    \"end\":         \"2022100100\"    # this is likely the wrong end date\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ed3e49",
   "metadata": {},
   "source": [
    "We define the function necessary to make the request to the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09cee263",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "#\n",
    "#    PROCEDURES/FUNCTIONS\n",
    "#\n",
    "\n",
    "def request_pageviews_per_article(article_title = None, \n",
    "                                  endpoint_url = API_REQUEST_PAGEVIEWS_ENDPOINT, \n",
    "                                  endpoint_params = API_REQUEST_PER_ARTICLE_PARAMS, \n",
    "                                  request_template = ARTICLE_PAGEVIEWS_PARAMS_TEMPLATE,\n",
    "                                  headers = REQUEST_HEADERS):\n",
    "    # Make sure we have an article title\n",
    "    if not article_title: return None\n",
    "    \n",
    "    # Titles are supposed to have spaces replaced with \"_\" and be URL encoded\n",
    "    article_title_encoded = urllib.parse.quote(article_title.replace(' ','_'))\n",
    "    request_template['article'] = article_title_encoded\n",
    "    \n",
    "    # now, create a request URL by combining the endpoint_url with the parameters for the request\n",
    "    request_url = endpoint_url+endpoint_params.format(**request_template)\n",
    "    \n",
    "    # make the request\n",
    "    try:\n",
    "        # we'll wait first, to make sure we don't exceed the limit in the situation where an exception\n",
    "        # occurs during the request processing - throttling is always a good practice with a free\n",
    "        # data source like Wikipedia - or other community sources\n",
    "        if API_THROTTLE_WAIT > 0.0:\n",
    "            time.sleep(API_THROTTLE_WAIT)\n",
    "        response = requests.get(request_url, headers=headers)\n",
    "        json_response = response.json()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        json_response = None\n",
    "    return json_response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4faa22d9",
   "metadata": {},
   "source": [
    "We take the list of dinosaur names that we want to search for from the input csv file, this file must be in the 'raw_data' folder. If the names have quotes inside, this procedure will correct them so that they are readable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c91ca951",
   "metadata": {},
   "outputs": [],
   "source": [
    "dinosaur_names = pd.read_csv('../data_raw/dinosaur_genera.cleaned.SEPT.2022 - dinosaur_genera.cleaned.SEPT.2022.csv')\n",
    "#quotes from excel are wrongly read, so we correct it\n",
    "dinosaur_names_list = dinosaur_names['name'].tolist()\n",
    "for i in range(len(dinosaur_names_list)):\n",
    "    if '“' in dinosaur_names_list[i]:\n",
    "        chars = '“”'\n",
    "        for c in chars:\n",
    "            dinosaur_names_list[i] = dinosaur_names_list[i].replace(c,'\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d3f1b2",
   "metadata": {},
   "source": [
    "For desktop, mobile and monthly cumulative we retrieve the data from the API using the functions declared previously. We then partially clean the data by removing the access field which is unnecessary for our analysis. Finally, we save the data into json files located in the 'data_clean' folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c60bdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#desktop data\n",
    "ARTICLE_PAGEVIEWS_PARAMS_TEMPLATE[\"access\"]=\"desktop\"\n",
    "desktop_dinosaur_dict={}\n",
    "for dinosaur_name in dinosaur_names_list:\n",
    "    months = request_pageviews_per_article(dinosaur_name)['items']\n",
    "    for month in months:\n",
    "        #We remove the 'access' field\n",
    "        del month['access']\n",
    "    #We assign the monthly information to the corresponding dino key inside the dictionary\n",
    "    desktop_dinosaur_dict[dinosaur_name] = months\n",
    "with open('../data_clean/dino_monthly_desktop_{}-{}.json'.format(ARTICLE_PAGEVIEWS_PARAMS_TEMPLATE[\"start\"][:6],ARTICLE_PAGEVIEWS_PARAMS_TEMPLATE[\"end\"][:6]), 'w', encoding='utf-8') as f:\n",
    "    #We save and transform the dictionary into JSON\n",
    "    json.dump(desktop_dinosaur_dict, f, ensure_ascii=False, indent=4)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1595375",
   "metadata": {},
   "source": [
    "For mobile data, we need to retrieve both mobile-web and mobile-app data, we sum these two to have only one count for all mobile activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9a878224",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mobile data\n",
    "mobile_dinosaur_dict={}\n",
    "for dinosaur_name in dinosaur_names_list:\n",
    "    ARTICLE_PAGEVIEWS_PARAMS_TEMPLATE[\"access\"]=\"mobile-web\"\n",
    "    mobile_web_months = request_pageviews_per_article(dinosaur_name)['items']\n",
    "    ARTICLE_PAGEVIEWS_PARAMS_TEMPLATE[\"access\"]=\"mobile-app\"\n",
    "    mobile_app_months = request_pageviews_per_article(dinosaur_name)['items']\n",
    "    #We make sure that for each dino, there is same amount of month information.\n",
    "    assert len(mobile_web_months) == len(mobile_app_months)\n",
    "    for i in range(len(mobile_web_months)):\n",
    "        #We make sure that we are summing over the same month\n",
    "        assert mobile_web_months[i]['timestamp'] ==  mobile_app_months[i]['timestamp'] \n",
    "        #We sum mobile-web and mobile-app views\n",
    "        mobile_web_months[i]['views'] = mobile_web_months[i]['views'] + mobile_app_months[i]['views']\n",
    "        #We remove the 'access' field\n",
    "        del mobile_web_months[i]['access']\n",
    "    mobile_months = mobile_web_months\n",
    "    #We assign the monthly information to the corresponding dino key inside the dictionary\n",
    "    mobile_dinosaur_dict[dinosaur_name] = mobile_months\n",
    "with open('../data_clean/dino_monthly_mobile_{}-{}.json'.format(ARTICLE_PAGEVIEWS_PARAMS_TEMPLATE[\"start\"][:6],ARTICLE_PAGEVIEWS_PARAMS_TEMPLATE[\"end\"][:6]), 'w', encoding='utf-8') as f:\n",
    "    #We save and transform the dictionary into JSON\n",
    "    json.dump(mobile_dinosaur_dict, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c837ef",
   "metadata": {},
   "source": [
    "From the API documentation, they mention the possibility to extract \"all-access\" data which is already the sum of desktop and mobile. To improve readability and similicity, we decide to take these instead of summing the two previous .json. We additionally calculate the requested cumulative sum and store this value in the 'views' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bdd09788",
   "metadata": {},
   "outputs": [],
   "source": [
    "#monthly cumulative\n",
    "ARTICLE_PAGEVIEWS_PARAMS_TEMPLATE[\"access\"]=\"all-access\"\n",
    "all_access_dinosaur_dict={}\n",
    "for dinosaur_name in dinosaur_names_list:\n",
    "    months = request_pageviews_per_article(dinosaur_name)['items']\n",
    "    cum_sum=0\n",
    "    for month in months:\n",
    "        #We generate the cumulative sum\n",
    "        cum_sum+=month['views']\n",
    "        month['views']=cum_sum\n",
    "        #We remove the 'access' field\n",
    "        del month['access']\n",
    "    all_access_dinosaur_dict[dinosaur_name] = months\n",
    "with open('../data_clean/dino_monthly_cumulative_{}-{}.json'.format(ARTICLE_PAGEVIEWS_PARAMS_TEMPLATE[\"start\"][:6],ARTICLE_PAGEVIEWS_PARAMS_TEMPLATE[\"end\"][:6]), 'w', encoding='utf-8') as f:\n",
    "    json.dump(all_access_dinosaur_dict, f, ensure_ascii=False, indent=4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
